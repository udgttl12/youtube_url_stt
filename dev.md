# 🎙️ YouTube 화자 분리 + STT 로컬 프로그램 개발 명세서

> **Windows / 무료 / GPU 자동 전환 / 고정밀**

---

## 1. 프로젝트 개요

### 1.1 목적

유튜브 영상 URL을 입력하면:

- 음성을 자동 추출하고
- 화자(발화자)를 분리한 뒤
- 각 화자의 발화를 정확하게 STT로 변환하여
- 사람이 읽기 쉬운 형태의 결과물로 저장하는

**로컬 실행 프로그램(exe)**을 개발한다.

### 1.2 핵심 제약 조건

- ✅ 완전 무료
- ✅ 로컬 처리 (외부 API 미사용)
- ✅ Windows OS
- ✅ GPU 있으면 자동 사용 / 없으면 CPU 자동 전환
- ✅ 정확도 최우선

---

## 2. 전체 시스템 아키텍처

### 2.1 처리 파이프라인

```text
[YouTube URL 입력]
        ↓
[영상 다운로드 (Audio Only)]
        ↓
[오디오 변환 및 전처리]
        ↓
[화자 분리 (Diarization)]
        ↓
[발화 구간 단위 STT]
        ↓
[화자 + 텍스트 병합]
        ↓
[결과 파일 생성]
```

---

## 3. 기능별 상세 명세

### 3.1 입력 기능

#### 3.1.1 입력 항목

- YouTube 영상 URL (단일 URL)
- 옵션 설정
  - **언어**: 자동 감지 / 한국어 고정
  - **화자 수**: 자동 / 수동 지정 (2~6명)
  - **출력 포맷 선택**: txt / srt / json
  - **정확도 모드**:
    - 표준
    - 고정밀 (속도 ↓, 정확도 ↑)

### 3.2 오디오 수집 모듈

#### 3.2.1 기능

- 유튜브 영상에서 최고 음질의 오디오 트랙만 다운로드
- 영상 파일은 저장하지 않음 (용량 절약)

#### 3.2.2 처리 정책

- 네트워크 오류 시 재시도
- 다운로드 실패 시 사용자에게 명확한 오류 메시지 제공

### 3.3 오디오 전처리 모듈

#### 3.3.1 변환 규격 (고정)

- **포맷**: WAV
- **샘플링 레이트**: 16,000 Hz
- **채널**: Mono

#### 3.3.2 전처리 항목

- 음량 정규화
- 무음 구간 정리
- 클리핑 방지

> 📌 전처리는 STT 정확도에 직접적인 영향을 미치므로 **필수 단계**로 간주한다.

### 3.4 하드웨어 감지 및 실행 모드 결정

#### 3.4.1 GPU / CPU 자동 전환 정책

프로그램 시작 시:

1. CUDA 사용 가능 여부 확인
2. 가능 → **GPU 모드**
3. 불가능 → **CPU 모드**로 자동 전환

#### 3.4.2 실행 모드 특성

| 항목 | GPU 모드 | CPU 모드 |
|------|----------|----------|
| 처리 속도 | 매우 빠름 | 느림 |
| 정확도 | 동일 | 동일 |
| 연산 타입 | FP16 | INT8 or FP32 |
| 사용 시점 | GPU 존재 시 | 기본 폴백 |

> ⚠️ 사용자는 GPU/CPU를 직접 선택하지 않으며, **자동 판단**이 원칙

### 3.5 화자 분리 (Speaker Diarization)

#### 3.5.1 목표

"누가 언제 말했는지"를 시간 단위로 구분

결과 예시:

```text
00:00:02 ~ 00:00:07 : SPEAKER_0
00:00:07 ~ 00:00:12 : SPEAKER_1
```

#### 3.5.2 우선 적용 방식 (정확도 기준)

- 딥러닝 기반 화자 분리 모델 사용
- 겹말(overlapping speech) 최대한 유지

#### 3.5.3 예외 처리 (Fallback 전략)

화자 분리 실패 시:

1. 단일 화자 모드로 자동 전환
2. 사용자에게 *"화자 분리 실패, 단일 화자로 처리됨"* 안내

### 3.6 STT (Speech-To-Text) 모듈

#### 3.6.1 기본 정책

- 전체 음성을 한 번에 STT **금지**
- 반드시 **화자 구간 단위** 또는 **발화 구간 단위**로 분할 후 STT 수행

#### 3.6.2 STT 처리 원칙

- 정확도 우선
- 단어 단위 타임스탬프 유지
- 언어 자동 감지 가능

#### 3.6.3 모델 전략

- **기본 모델**: 대형 고정밀 모델
- CPU-only 환경에서 너무 느릴 경우:
  - 중간급 모델로 자동 다운그레이드 옵션 제공

### 3.7 결과 병합 로직

#### 3.7.1 병합 규칙

STT 결과를 화자 구간에 매핑

출력 예시:

```text
[SPEAKER_0] 안녕하세요 오늘 회의 시작하겠습니다.
[SPEAKER_1] 네, 자료 공유드리겠습니다.
```

#### 3.7.2 시간 정보

- 모든 발화에는 시작/종료 시간 포함 가능
- SRT/VTT 출력 시 자동 반영

### 3.8 출력 파일 명세

#### 3.8.1 TXT

- 사람 읽기용 회의록 / 대담 기록

#### 3.8.2 SRT

- 자막 형식
- 화자 라벨 포함

#### 3.8.3 JSON

- 구조화 데이터
- 예시 구조:

```json
{
  "speaker": "화자 ID",
  "start": "시작 시간",
  "end": "종료 시간",
  "text": "텍스트"
}
```

> 📌 JSON은 후속 처리(요약, 검색, ERP 저장)에 사용 가능

### 3.9 UI 명세 (exe 기준)

#### 3.9.1 필수 UI 요소

- URL 입력창
- 옵션 설정 패널
- 진행 상태 표시 (단계별)
  - 다운로드
  - 전처리
  - 화자 분리
  - STT
- 로그 출력 창
- 결과 미리보기
- 파일 저장 버튼

#### 3.9.2 UX 원칙

- 현재 어떤 단계인지 **항상 명확히 표시**
- 장시간 처리 시 "멈춘 것처럼 보이지 않게" **진행률 표시 필수**

### 3.10 배포 및 exe 패키징

#### 3.10.1 배포 방식

- Windows exe 단일 실행 파일
- 최초 실행 시:
  - 모델 자동 다운로드
  - 이후 캐시 재사용

#### 3.10.2 포함 요소

- ffmpeg 실행 파일
- 내부 리소스 경로 관리

#### 3.10.3 주의 사항

- exe 용량 폭증 방지
- 모델은 exe 내부에 **포함하지 않음**

---

## 4. 단계별 개발 로드맵

| Phase | 내용 |
|-------|------|
| **Phase 1** | 유튜브 URL → STT 텍스트 출력 |
| **Phase 2** | 발화 구간 분할 + STT 정확도 향상 |
| **Phase 3** | 화자 분리 적용 |
| **Phase 4** | UI + exe 패키징 |
| **Phase 5** | 출력 포맷 확장 및 안정화 |

---

## 5. 품질 기준 (Acceptance Criteria)

- 화자 수 2명 대담 영상에서 화자 라벨 정확도 **체감상 80% 이상**
- 한국어 STT 오탈자 최소화
- GPU/CPU 환경에서 모두 정상 동작
- exe 실행 후 추가 설치 요구 없음

---

## 6. 리스크 및 대응 전략

| 리스크 | 대응 |
|--------|------|
| CPU 처리 속도 | 모델 다운그레이드 옵션 |
| 화자 분리 실패 | 단일 화자 폴백 |
| ffmpeg 경로 오류 | 내부 상대 경로 고정 |
| 긴 영상 | 분할 처리 |

---

## 7. 최종 한 줄 요약

> **"유튜브 링크 하나로, 누가 언제 뭐라고 말했는지 정확하게 뽑아주는 로컬 exe"**

쓸데없는 기능 없이, 정확도에만 모든 스탯 찍은 설계다 💪
(민첩성은 포기, 지능 몰빵 캐릭터)
